# ğŸ›’ GuÃ­a Completa: Scraping Carulla para LecFac

## ğŸ“‹ Ãndice
1. [InstalaciÃ³n](#instalaciÃ³n)
2. [Uso BÃ¡sico](#uso-bÃ¡sico)
3. [IntegraciÃ³n con LecFac](#integraciÃ³n-con-lecfac)
4. [Consideraciones Legales](#consideraciones-legales)
5. [Buenas PrÃ¡cticas](#buenas-prÃ¡cticas)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ”§ InstalaciÃ³n

### 1. Instalar Playwright

```bash
# Instalar Playwright
pip install playwright --break-system-packages

# Instalar navegadores
playwright install chromium
```

### 2. Verificar archivos

Debes tener estos 3 archivos:
- `carulla_scraper.py` - Scraper base
- `lecfac_enricher.py` - IntegraciÃ³n con LecFac
- `GUIA_SCRAPING.md` - Este archivo

---

## ğŸš€ Uso BÃ¡sico

### Ejemplo 1: Scrapear un producto especÃ­fico

```python
import asyncio
from carulla_scraper import scrapear_url

# URL de un producto
url = "https://www.carulla.com/queso-fresco-15-pcto-de-descuento-paq-x-30-tajadas-559646/p"

# Ejecutar
producto = asyncio.run(scrapear_url(url))

print(producto)
# Output:
# {
#     'nombre': 'Queso Mozarella FINESSE 30 tajadas (450 gr)',
#     'plu': '426036',
#     'precio': 26100,
#     'supermercado': 'Carulla',
#     'url': 'https://...'
# }
```

### Ejemplo 2: Buscar productos

```python
import asyncio
from carulla_scraper import buscar_productos

# Buscar "queso mozarella" en Carulla
productos = asyncio.run(buscar_productos("queso mozarella", max_productos=5))

for p in productos:
    print(f"{p['nombre']} - PLU: {p['plu']} - ${p['precio']:,}")
```

---

## ğŸ”— IntegraciÃ³n con LecFac

### Flujo Recomendado

```
OCR Extrae Producto
        â†“
Â¿Existe en BD?
    â†“ NO
Scrapear Carulla
        â†“
Enriquecer con datos completos
        â†“
Guardar en BD
```

### CÃ³digo de IntegraciÃ³n

```python
from lecfac_enricher import ProductEnricher
import asyncio

async def procesar_producto_ocr(producto_ocr):
    """
    Procesa un producto extraÃ­do por OCR
    """
    enricher = ProductEnricher()
    
    # Producto del OCR
    producto = {
        'nombre': 'MOZARELL FINESSE',  # Nombre parcial del OCR
        'plu': '426036',
        'precio': 26100,
        'supermercado': 'Carulla'
    }
    
    # Enriquecer con scraping
    producto_enriquecido = await enricher.enriquecer_producto_lecfac(producto)
    
    # Ahora tienes:
    print(producto_enriquecido['nombre_completo'])
    # 'Queso Mozarella FINESSE 30 tajadas (450 gr)'
    
    return producto_enriquecido

# Ejecutar
asyncio.run(procesar_producto_ocr(...))
```

### IntegraciÃ³n con FastAPI

```python
# En tu backend (productos_api_v2.py o similar)

from lecfac_enricher import ProductEnricher

enricher = ProductEnricher()

@app.post("/api/productos/enriquecer")
async def enriquecer_producto(producto: dict):
    """
    Endpoint para enriquecer productos con scraping
    """
    try:
        producto_enriquecido = await enricher.enriquecer_producto_lecfac(producto)
        return {
            "success": True,
            "data": producto_enriquecido
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }
```

---

## âš–ï¸ Consideraciones Legales

### âœ… PrÃ¡cticas Seguras

1. **Rate Limiting Estricto**
   - 3 segundos entre requests (ya implementado)
   - MÃ¡ximo 100 productos/dÃ­a por IP
   - Solo productos que no existen en tu BD

2. **Caching Agresivo**
   ```python
   # Guardar productos scrapeados en tu BD
   # NO scrapear el mismo producto dos veces
   ```

3. **User-Agent Honesto**
   ```python
   headers = {
       'User-Agent': 'LecFac Price Comparison Bot (+contacto@lecfac.com)'
   }
   ```

4. **Respetar robots.txt**
   - Revisar: https://www.carulla.com/robots.txt
   - No scrapear Ã¡reas prohibidas

### âš ï¸ Zona Gris Legal

**Scraping en Colombia:**
- âœ… Datos pÃºblicos (precios, nombres)
- âœ… Uso personal/investigaciÃ³n
- âš ï¸ Uso comercial (tu caso)
- âŒ Replicar el sitio completo
- âŒ Competir directamente

**RecomendaciÃ³n:**
1. **Contactar a Carulla primero** (mejores prÃ¡cticas)
2. Scraping como fallback temporal
3. Buscar API oficial en paralelo

---

## ğŸ¯ Buenas PrÃ¡cticas

### 1. CuÃ¡ndo Scrapear

âœ… **SÃ scrapear:**
- Producto nuevo (no existe en BD)
- PLU sin nombre completo
- Precios desactualizados (>7 dÃ­as)

âŒ **NO scrapear:**
- Productos que ya tienes completos
- Cada vez que usuario escanea factura
- Consultas repetitivas

### 2. ImplementaciÃ³n PrÃ¡ctica

```python
async def debe_scrapear(producto_plu: str) -> bool:
    """
    Determina si vale la pena scrapear
    """
    # Buscar en BD
    producto_bd = buscar_en_bd(producto_plu)
    
    if not producto_bd:
        return True  # Producto nuevo
    
    if not producto_bd.get('nombre_completo'):
        return True  # Falta info
    
    if (datetime.now() - producto_bd['ultima_actualizacion']).days > 7:
        return True  # Desactualizado
    
    return False  # Ya tenemos buena info
```

### 3. Cola de Scraping AsÃ­ncrona

```python
# En lugar de scrapear en tiempo real:
# 1. Usuario escanea factura
# 2. OCR procesa
# 3. Agregar productos a cola de scraping
# 4. Procesar cola cada noche (off-peak hours)

import asyncio
from datetime import datetime

scraping_queue = []

def agregar_a_cola(producto):
    """Agregar producto a cola de scraping"""
    scraping_queue.append({
        'producto': producto,
        'timestamp': datetime.now()
    })

async def procesar_cola_nocturna():
    """Procesar cola de scraping (ejecutar a las 2 AM)"""
    enricher = ProductEnricher()
    
    for item in scraping_queue:
        producto = await enricher.enriquecer_producto_lecfac(item['producto'])
        guardar_en_bd(producto)
        
        await asyncio.sleep(5)  # Rate limiting generoso
    
    scraping_queue.clear()
```

### 4. Manejo de Errores

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def scrapear_con_reintentos(url):
    """Reintentar scraping en caso de fallo"""
    return await scraper.scrape_producto(url)
```

---

## ğŸ” Troubleshooting

### Problema 1: "playwright not found"

```bash
# Reinstalar
pip uninstall playwright
pip install playwright --break-system-packages
playwright install chromium
```

### Problema 2: PÃ¡gina no carga

```python
# Aumentar timeout
await page.goto(url, wait_until="networkidle", timeout=30000)
await page.wait_for_timeout(5000)  # 5 segundos
```

### Problema 3: No encuentra precios

```python
# El precio puede estar en diferentes elementos
# Revisar manualmente la pÃ¡gina y ajustar los selectores:
precio = await page.locator("[data-testid='price']").text_content()
```

### Problema 4: Rate limiting / Bloqueo IP

```python
# Soluciones:
# 1. Aumentar delay entre requests
self.rate_limit_delay = 5  # 5 segundos

# 2. Rotar User-Agent
# 3. Usar proxies (avanzado)
# 4. Contactar Carulla para API oficial
```

---

## ğŸ“Š MÃ©tricas Sugeridas

```python
# Trackear efectividad del scraping
metricas = {
    'productos_scrapeados': 0,
    'productos_exitosos': 0,
    'productos_fallidos': 0,
    'tiempo_promedio': 0,
    'errores_por_tipo': {}
}

# Revisar semanalmente:
# - Tasa de Ã©xito (debe ser >80%)
# - Productos Ãºnicos (no duplicar scraping)
# - Uso de cache (debe ser >70%)
```

---

## ğŸš¦ Siguiente Paso

### OpciÃ³n A: Empezar PequeÃ±o (Recomendado)

```bash
# 1. Probar el scraper con 3-5 productos manualmente
python carulla_scraper.py

# 2. Si funciona bien, integrar en flujo de OCR
# 3. Monitorear por 1 semana
# 4. Escalar gradualmente
```

### OpciÃ³n B: Contactar Carulla Primero

```
Asunto: Propuesta de ColaboraciÃ³n - App ComparaciÃ³n de Precios

Estimados,

Soy desarrollador de LecFac, una app comunitaria que ayuda a 
colombianos a encontrar mejores precios en supermercados.

Â¿Carulla ofrece una API de productos para desarrolladores?

Estamos dispuestos a:
- Reconocer a Carulla como fuente oficial
- Enlazar directamente al sitio
- Cumplir con tÃ©rminos de uso

Contacto: santiago@lecfac.com
```

---

## ğŸ“ Resumen: Pros y Contras

### âœ… Ventajas del Scraping

- Datos completos y actualizados
- ImplementaciÃ³n rÃ¡pida
- Gratuito
- Control total

### âš ï¸ Desventajas del Scraping

- Zona gris legal
- Puede romperse si cambia el sitio
- Riesgo de bloqueo
- Mantenimiento necesario

### ğŸ’¡ RecomendaciÃ³n Final

**Plan Dual:**
1. Implementar scraping como MVP (esta semana)
2. Contactar Carulla para API oficial (en paralelo)
3. Migrar a API cuando estÃ© disponible

---

## ğŸ“ Soporte

Â¿Dudas? Revisa:
- DocumentaciÃ³n Playwright: https://playwright.dev/python/
- TÃ©rminos de Carulla: https://www.carulla.com/terminos-y-condiciones
- Contacto Carulla: servicio al cliente

---

**Ãšltima actualizaciÃ³n:** 25 Nov 2024  
**VersiÃ³n:** 1.0  
**Autor:** Claude + Santiago
